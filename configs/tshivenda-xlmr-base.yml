# Configuration for training a multilingual language model using the XLM-RoBERTa architecture
# Ref: https://huggingface.co/docs/transformers/model_doc/xlm-roberta
common:
  model_name: tshivenda-xlmr-base
  experiment_path: data/models/pretrained
  log_level: INFO
  evaluate_only: False
  train_only: False
  random_seed: 47
  gpu_count: 1
  default_xlm_checkpoint: xlm-roberta-base
  train_from_scratch: True
  resume_training: False
  use_whole_word_mask: False
  # The likelihood of a token being train_masked in the input text. Higher values will mask more tokens
  mlm_probability: 0.15
  # Controls under-samplling or over-sampling of languages in the training data. e.g. 0.5 means use half the data
  language_sampling_factor: 1.0
  input_text_encoding: utf-8
  minimum_tokens_per_sentence: 5

# XLMRobertaConfig - used when initialising model to train from scratch
# Ref: https://huggingface.co/docs/transformers/model_doc/xlm-roberta#transformers.XLMRobertaConfig
model:
  tokenizer_path: data/tokenizers/tshivenda-xlmr-30k
  layer_norm_eps: 1.0e-05
  output_past: True
  type_vocab_size: 1
  max_length: 512
  hidden_size: 768
  num_attention_heads: 6
  num_hidden_layers: 8
  intermediate_size: 3072

# Arguments to be passed to hugging face TrainingArguments instance
# Ref: https://huggingface.co/docs/transformers/v4.39.2/en/main_classes/trainer#transformers.TrainingArguments
training:
  output_dir: data/models/tshivenda-xlmr-30k # Can be overriden by command line argument
  gradient_accumulation_steps: 8
  ignore_data_skip: False
  overwrite_output_dir: False
  seed: 47
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 16
  fp16: True
  save_steps: 1
  save_strategy: epoch
  save_total_limit: 1
  logging_steps: 100
  learning_rate: 0.0001
  num_train_epochs: 20
  log_level: info
  log_level_replica: info
  load_best_model_at_end: True
  evaluation_strategy: epoch
  metric_for_best_model: eval_loss
  # Change this as needed - Convenient way to save your checkpoints to 
  # Hugging Face Hub every N steps. Set to False to disable.
  push_to_hub: False
  hub_model_id: 'ndamulelonemakh/tshivenda-xlmr-base'

# Working directory for training and evaluation data
# Note: We use a dedicated working directory for each model to avoid multi-process conflicts
#      when training multiple models in parallel
# Furthemore, for multilingual models, we need to collect multiple languages in a single directory.
# For very large datasets, consider using a symlink to the original data to avoid copying the data
data:
  train: data/tmp/tshivenda-xlmr-base/train                   # Training data directory. File patten is train.{lang} eg. train.ven
  eval_all: data/tmp/tshivenda-xlmr-base/eval/full_eval.txt   # Evaluate language model on dataset with all languages
  eval_for_lang: data/tmp/tshivenda-xlmr-base/eval            # Evaluation data directory. File patten is eval.{lang} eg. eval.ven
  filter_langs: ven                                           # Pick which languages will be used to train. Use all to train on all languages
  tokenization_sampling_factor: 0.5
  train_file_pattern: train.*
  eval_file_pattern: eval.*
  train_eval_split_ratio: 0.2
  normalize_text: False
  overwrite_cache: False
  rechunk: False  # When True, alll text will be merged then split into chunks of max_length
