
vars:  # Usage examples: https://dvc.org/doc/user-guide/project-structure/dvcyaml-files#variables
  - desc: 'Zabantu Masked Language Modeling pipelines'
  - root_dir: '/home/ndamulelo/projects/mit-807-2022-ndamulelo'

  - base_model_epochs: 10

  - mlm_train_in_dir: 'data/interim/bantu'
  - mlm_scripts_dir: 'zabantu_mlm'

  - zabantu_tshivenda_model: 'data/models/zabantu/tshivenda'
  - zabantu_tshivenda_tokenizer: 'data/tokenizers/zabantu/tshivenda'

  - zabantu_sepedi_model: 'data/models/zabantu/sepedi'
  - zabantu_sepedi_tokenizer: 'data/tokenizers/zabantu/sepedi'

  - zabantu_blmr_nso_ven_model: 'data/models/zabantu/blmr_nso_ven'
  - zabantu_blmr_nso_ven_tokenizer: 'data/tokenizers/zabantu/blmr_nso_ven'

  - zabantu_xlmr_sot_ven_model: 'data/models/zabantu/xlmr_sot_ven'
  - zabantu_xlmr_sot_ven_tokenizer: 'data/tokenizers/zabantu/xlmr_sot_ven'

  - zabantu_xlmr_model: 'data/models/zabantu/xlmr_bantu'
  - zabantu_xlmr_tokenizer: 'data/tokenizers/zabantu/xlmr_bantu'

  - comet_workspace: 'zabantu2023/zabantu-language-models'


# Note: By default, there is no preprocessing done on the raw data except normalisng to ascii
# Going forward: It may be interesting to try out different preprocessing steps and
# evaluate the impact on the model performance

stages:
  debug:
    desc: Test if the pipeline is working
    cmd: echo "Masked Language Modeling pipeline is working"

  train_tshivenda_tokenizers:
    foreach:
      - model_type: unigram
        vocab_size: 30000
        vocab_size_code: 30k
      # We currently dont have enough data to train a 50k and 70k unigram tokenizers..
      # - model_type: unigram
      #   vocab_size: 50000
      #   vocab_size_code: 50k
      # - model_type: unigram
      #   vocab_size: 70000
      #   vocab_size_code: 70k


      - model_type: bpe
        vocab_size: 30000
        vocab_size_code: 30k
      - model_type: bpe
        vocab_size: 50000
        vocab_size_code: 50k
      - model_type: bpe
        vocab_size: 70000
        vocab_size_code: 70k
      - model_type: bpe
        vocab_size: 85000
        vocab_size_code: 85k

    do:
      cmd: >-
        cd ${root_dir} &&
        python ${mlm_scripts_dir}/train_native_sp_tokenizer.py
        --input_data_paths ${root_dir}/${mlm_train_in_dir}/bantu.ven.txt
        --model_type ${item.model_type}
        --name sentencepiece.bpe
        --vocab_size ${item.vocab_size}
        --output_dir ${root_dir}/${zabantu_tshivenda_tokenizer}/zabantu_ven_sent_${item.model_type}_tokenizer_${item.vocab_size_code}
        --normalize
        --overwrite
        --eval_text "Ndi matsheloni vhathetshelesi! Vho tanganedzwa."
      deps:
        - ${root_dir}/${mlm_train_in_dir}/bantu.ven.txt
        - ${root_dir}/${mlm_scripts_dir}/train_native_sp_tokenizer.py
      outs:
        - ${root_dir}/${zabantu_tshivenda_tokenizer}/zabantu_ven_sent_${item.model_type}_tokenizer_${item.vocab_size_code}

  train_sepedi_tokenizers:
    foreach:
      - model_type: unigram
        vocab_size: 30000
        vocab_size_code: 30k
      - model_type: unigram
        vocab_size: 50000
        vocab_size_code: 50k
      - model_type: unigram
        vocab_size: 70000
        vocab_size_code: 70k

      - model_type: bpe
        vocab_size: 30000
        vocab_size_code: 30k
      - model_type: bpe
        vocab_size: 50000
        vocab_size_code: 50k
      - model_type: bpe
        vocab_size: 70000
        vocab_size_code: 70k
      - model_type: bpe
        vocab_size: 85000
        vocab_size_code: 85k
    do:
      cmd: >-
        cd ${root_dir} &&
        python ${mlm_scripts_dir}/train_native_sp_tokenizer.py
        --input_data_paths ${root_dir}/${mlm_train_in_dir}/bantu.nso.txt
        --model_type ${item.model_type}
        --name sentencepiece.bpe
        --vocab_size ${item.vocab_size}
        --output_dir ${root_dir}/${zabantu_sepedi_tokenizer}/zabantu_nso_sent_${item.model_type}_tokenizer_${item.vocab_size_code}
        --normalize
        --overwrite
        --eval_text "Laetša ge eba ba na le kgahlego ka go protšeke le/goba ge eba ba tla amega ka go holega;"
      deps:
        - ${root_dir}/${mlm_train_in_dir}/bantu.nso.txt
        - ${root_dir}/${mlm_scripts_dir}/train_native_sp_tokenizer.py
      outs:
        - ${root_dir}/${zabantu_sepedi_tokenizer}/zabantu_nso_sent_${item.model_type}_tokenizer_${item.vocab_size_code}

  train_nso_ven_tokenizers:
    foreach:
      # - model_type: unigram
      #   vocab_size: 30000
      #   vocab_size_code: 30k
      # - model_type: unigram
      #   vocab_size: 50000
      #   vocab_size_code: 50k
      # - model_type: unigram
      #   vocab_size: 70000
      #   vocab_size_code: 70k
      # - model_type: unigram
      #   vocab_size: 30000
      #   vocab_size_code: 30k
      # - model_type: unigram
      #   vocab_size: 50000
      #   vocab_size_code: 50k
      # - model_type: unigram
      #   vocab_size: 70000
      #   vocab_size_code: 70k
      - model_type: unigram
        vocab_size: 85000
        vocab_size_code: 85k

      # - model_type: bpe
      #   vocab_size: 30000
      #   vocab_size_code: 30k
      # - model_type: bpe
      #   vocab_size: 50000
      #   vocab_size_code: 50k
      # - model_type: bpe
      #   vocab_size: 70000
      #   vocab_size_code: 70k
      - model_type: bpe
        vocab_size: 150000
        vocab_size_code: 150k
      - model_type: bpe
        vocab_size: 250000
        vocab_size_code: 250k
    do:
      desc: Train a tokenizer for Sepedi and Tshivenda
      cmd: >-
        cd ${root_dir} &&
        python ${mlm_scripts_dir}/train_native_sp_tokenizer.py
        --input_data_paths ${root_dir}/${mlm_train_in_dir}/bantu.nso.txt,${root_dir}/${mlm_train_in_dir}/bantu.ven.txt
        --model_type ${item.model_type}
        --name sentencepiece.bpe
        --vocab_size ${item.vocab_size}
        --output_dir ${root_dir}/${zabantu_blmr_nso_ven_tokenizer}/zabantu_nso_ven_sent_${item.model_type}_tokenizer_${item.vocab_size_code}
        --normalize
        --overwrite
        --eval_text "Laetša ge eba ba na le kgahlego ka go protšeke le/goba ge eba ba tla amega ka go holega; Ndi matsheloni vhathetshelesi! Vho tanganedzwa."
      deps:
        - ${root_dir}/${mlm_train_in_dir}/bantu.nso.txt
        - ${root_dir}/${mlm_train_in_dir}/bantu.ven.txt
        - ${root_dir}/${mlm_scripts_dir}/train_native_sp_tokenizer.py
      outs:
        - ${root_dir}/${zabantu_blmr_nso_ven_tokenizer}/zabantu_nso_ven_sent_${item.model_type}_tokenizer_${item.vocab_size_code}

  train_ven_sotho_family_tokenizers:
    foreach:
      # - model_type: unigram
      #   vocab_size: 30000
      #   vocab_size_code: 30k
      # - model_type: unigram
      #   vocab_size: 50000
      #   vocab_size_code: 50k
      # - model_type: unigram
      #   vocab_size: 70000
      #   vocab_size_code: 70k
      - model_type: unigram
        vocab_size: 85000
        vocab_size_code: 85k
      - model_type: unigram
        vocab_size: 150000
        vocab_size_code: 150k

      # - model_type: bpe
      #   vocab_size: 30000
      #   vocab_size_code: 30k
      # - model_type: bpe
      #   vocab_size: 50000
      #   vocab_size_code: 50k
      # - model_type: bpe
      #   vocab_size: 70000
      #   vocab_size_code: 70k
      - model_type: bpe
        vocab_size: 85000
        vocab_size_code: 85k
      - model_type: bpe
        vocab_size: 150000
        vocab_size_code: 150k
      - model_type: bpe
        vocab_size: 250000
        vocab_size_code: 250k

    do:
      desc: "Train sentencepiece tokenizer Tshivenda and all Sotho family languages"
      cmd: >-
        cd ${root_dir} &&
        python ${mlm_scripts_dir}/train_native_sp_tokenizer.py
        --input_data_paths ${root_dir}/${mlm_train_in_dir}/bantu.nso.txt,${root_dir}/${mlm_train_in_dir}/bantu.ven.txt,${root_dir}/${mlm_train_in_dir}/bantu.tsn.txt,${root_dir}/${mlm_train_in_dir}/bantu.sot.txt
        --model_type ${item.model_type}
        --name sentencepiece.bpe
        --vocab_size ${item.vocab_size}
        --output_dir ${root_dir}/${zabantu_xlmr_sot_ven_tokenizer}/zabantu_sot_ven_sent_${item.model_type}_tokenizer_${item.vocab_size_code}
        --normalize
        --overwrite
        --eval_text "Laetša ge eba ba na le kgahlego ka go protšeke le/goba ge eba ba tla amega ka go holega; Ndi matsheloni vhathetshelesi! Vho tanganedzwa."
      deps:
        - ${root_dir}/${mlm_train_in_dir}/bantu.nso.txt
        - ${root_dir}/${mlm_train_in_dir}/bantu.tsn.txt
        - ${root_dir}/${mlm_train_in_dir}/bantu.sot.txt
        - ${root_dir}/${mlm_train_in_dir}/bantu.ven.txt
        - ${root_dir}/${mlm_scripts_dir}/train_native_sp_tokenizer.py
      outs:
        - ${root_dir}/${zabantu_xlmr_sot_ven_tokenizer}/zabantu_sot_ven_sent_${item.model_type}_tokenizer_${item.vocab_size_code}

  train_zabantu_xlmr_tokenizers:
    foreach:
      # - model_type: unigram
      #   vocab_size: 30000
      #   vocab_size_code: 30k
      # - model_type: unigram
      #   vocab_size: 50000
      #   vocab_size_code: 50k
      # - model_type: unigram
      #   vocab_size: 70000
      #   vocab_size_code: 70k
      - model_type: unigram
        vocab_size: 85000
        vocab_size_code: 85k
      - model_type: unigram
        vocab_size: 150000
        vocab_size_code: 150k
      # - model_type: unigram
      #   vocab_size: 250000
      #   vocab_size_code: 250k

      # - model_type: bpe
      #   vocab_size: 30000
      #   vocab_size_code: 30k
      # - model_type: bpe
      #   vocab_size: 50000
      #   vocab_size_code: 50k
      # - model_type: bpe
      #   vocab_size: 70000
      #   vocab_size_code: 70k
      - model_type: bpe
        vocab_size: 85000
        vocab_size_code: 85k
      - model_type: bpe
        vocab_size: 150000
        vocab_size_code: 150k
      # - model_type: bpe
      #   vocab_size: 250000
      #   vocab_size_code: 250k
    do:
      desc: "Training ZABANTU XLMR tokenizer for ${item.model_type} model with vocab size ${item.vocab_size}"
      cmd: >-
        cd ${root_dir} &&
        python ${mlm_scripts_dir}/train_native_sp_tokenizer.py
        --input_data_paths "${root_dir}/${mlm_train_in_dir}/bantu.*.txt"
        --model_type ${item.model_type}
        --name sentencepiece.bpe
        --vocab_size ${item.vocab_size}
        --output_dir ${root_dir}/${zabantu_xlmr_tokenizer}/zabantu_xlmr_sent_${item.model_type}_tokenizer_${item.vocab_size_code}
        --normalize
        --overwrite
        --eval_text "Laetša ge eba ba na le kgahlego ka go protšeke le/goba ge eba ba tla amega ka go holega; Ndi matsheloni vhathetshelesi! Vho tanganedzwa."
      deps:
        - ${root_dir}/${mlm_train_in_dir}/bantu.nso.txt
        - ${root_dir}/${mlm_train_in_dir}/bantu.tsn.txt
        - ${root_dir}/${mlm_train_in_dir}/bantu.sot.txt
        - ${root_dir}/${mlm_train_in_dir}/bantu.ven.txt
        - ${root_dir}/${mlm_scripts_dir}/train_native_sp_tokenizer.py
      outs:
        - ${root_dir}/${zabantu_xlmr_tokenizer}/zabantu_xlmr_sent_${item.model_type}_tokenizer_${item.vocab_size_code}

  train_tshivenda_language_model_base:
    foreach:
      - model_type: unigram
        vocab_size: 30k

      - model_type: bpe
        vocab_size: 30k
      - model_type: bpe
        vocab_size: 50k
      - model_type: bpe
        vocab_size: 70k
    do:
      desc: Train a Tshivenda language model using the XLM-R architecture
      vars:
        - lang: ven
      cmd: >-
        cd ${root_dir} &&
        bash ${root_dir}/${mlm_scripts_dir}/run_xlm_train_pipeline.sh 
        ${root_dir}/${mlm_scripts_dir}/zabantu_ven_base.yml                                                
        zabantu-${lang}-${item.vocab_size}-${item.model_type}-${base_model_epochs}epochs           
        ${mlm_train_in_dir}                                                               
        zabantu-lm-train-ven-$RANDOM                                                    
        ${root_dir}/${zabantu_tshivenda_tokenizer}/zabantu_ven_sent_${item.model_type}_tokenizer_${item.vocab_size}
        ${base_model_epochs}
      deps:
        - ${root_dir}/${zabantu_tshivenda_tokenizer}/zabantu_ven_sent_${item.model_type}_tokenizer_${item.vocab_size}
      outs:
        - ${root_dir}/data/models/zabantu-${lang}-${item.vocab_size}-${item.model_type}-${base_model_epochs}epochs

  train_tshivenda_language_models:
    foreach:
      - model_type: unigram
        vocab_size: 30k
        epochs: 20
      - model_type: unigram
        vocab_size: 30k
        epochs: 50
      - model_type: unigram
        vocab_size: 30k
        epochs: 100

      - model_type: bpe
        vocab_size: 30k
        epochs: 20
      - model_type: bpe
        vocab_size: 30k
        epochs: 50
      - model_type: bpe
        vocab_size: 30k
        epochs: 100

      - model_type: bpe
        vocab_size: 50k
        epochs: 20
      - model_type: bpe
        vocab_size: 50k
        epochs: 50
      - model_type: bpe
        vocab_size: 50k
        epochs: 100

      - model_type: bpe
        vocab_size: 70k
        epochs: 20
      - model_type: bpe
        vocab_size: 70k
        epochs: 50
      - model_type: bpe
        vocab_size: 70k
        epochs: 100

    do:
      desc: Train misc Tshivenda language model using the XLM-R architecture
      vars:
        - lang: ven
      cmd: >-
        cd ${root_dir} &&
        bash ${mlm_scripts_dir}/run_xlm_train_pipeline.sh 
        ${root_dir}/${mlm_scripts_dir}/zabantu_ven_base.yml                                                
        zabantu-${lang}-${item.vocab_size}-${item.model_type}-${item.epochs}epochs           
        ${mlm_train_in_dir}                                                               
        zabantu-lm-train-ven-$RANDOM                                                    
        ${root_dir}/${zabantu_tshivenda_tokenizer}/zabantu_ven_sent_${item.model_type}_tokenizer_${item.vocab_size}
        ${item.epochs}
      deps:
        - ${root_dir}/${zabantu_tshivenda_tokenizer}/zabantu_ven_sent_${item.model_type}_tokenizer_${item.vocab_size}
      outs:
        - ${root_dir}/data/models/zabantu-${lang}-${item.vocab_size}-${item.model_type}-${item.epochs}epochs

  train_sepedi_language_model_base:
    foreach:
      - model_type: unigram
        vocab_size: 30k
      - model_type: unigram
        vocab_size: 50k
      - model_type: unigram
        vocab_size: 70k

      - model_type: bpe
        vocab_size: 30k
      - model_type: bpe
        vocab_size: 50k
      - model_type: bpe
        vocab_size: 70k
    do:
      desc: Train a Sepedi language model using the XLM-R architecture
      vars:
        - lang: nso
      cmd: >-
        cd ${root_dir} &&
        bash ${root_dir}/${mlm_scripts_dir}/run_xlm_train_pipeline.sh 
        ${root_dir}/${mlm_scripts_dir}/zabantu_nso_base.yml                                                
        zabantu-${lang}-${item.vocab_size}-${item.model_type}-${base_model_epochs}epochs           
        ${mlm_train_in_dir}                                                               
        zabantu-lm-train-${lang}-$RANDOM                                                    
        ${root_dir}/${zabantu_sepedi_tokenizer}/zabantu_nso_sent_${item.model_type}_tokenizer_${item.vocab_size}
        ${base_model_epochs}
      deps:
        - ${root_dir}/${zabantu_sepedi_tokenizer}/zabantu_nso_sent_${item.model_type}_tokenizer_${item.vocab_size}
      outs:
        - ${root_dir}/data/models/zabantu-${lang}-${item.vocab_size}-${item.model_type}-${base_model_epochs}epochs

  train_sepedi_language_models:
    foreach:
      - model_type: unigram
        vocab_size: 30k
        epochs: 20
      # - model_type: unigram
      #   vocab_size: 30k
      #   epochs: 50
      - model_type: unigram
        vocab_size: 30k
        epochs: 100

      - model_type: unigram
        vocab_size: 50k
        epochs: 20
      # - model_type: unigram
      #   vocab_size: 50k
      #   epochs: 50
      - model_type: unigram
        vocab_size: 50k
        epochs: 100

      - model_type: unigram
        vocab_size: 70k
        epochs: 20
      # - model_type: unigram
      #   vocab_size: 70k
      #   epochs: 50
      - model_type: unigram
        vocab_size: 70k
        epochs: 100

      - model_type: bpe
        vocab_size: 30k
        epochs: 20
      # - model_type: bpe
      #   vocab_size: 30k
      #   epochs: 50
      - model_type: bpe
        vocab_size: 30k
        epochs: 100

      - model_type: bpe
        vocab_size: 50k
        epochs: 20
      # - model_type: bpe
      #   vocab_size: 50k
      #   epochs: 50
      - model_type: bpe
        vocab_size: 50k
        epochs: 100

      - model_type: bpe
        vocab_size: 70k
        epochs: 20
      # - model_type: bpe
      #   vocab_size: 70k
      #   epochs: 50
      - model_type: bpe
        vocab_size: 70k
        epochs: 100

    do:
      desc: Train misc Sepedi language models using the XLM-R architecture
      vars:
        - lang: nso
      cmd: >-
        cd ${root_dir} &&
        bash ${mlm_scripts_dir}/run_xlm_train_pipeline.sh 
        ${root_dir}/${mlm_scripts_dir}/zabantu_nso_base.yml                                               
        zabantu-${lang}-${item.vocab_size}-${item.model_type}-${base_model_epochs}epochs           
        ${mlm_train_in_dir}                                                               
        zabantu-lm-train-${lang}-$RANDOM                                                    
        ${root_dir}/${zabantu_sepedi_tokenizer}/zabantu_nso_sent_${item.model_type}_tokenizer_${item.vocab_size}
        ${item.epochs}
      deps:
        - ${root_dir}/${zabantu_sepedi_tokenizer}/zabantu_nso_sent_${item.model_type}_tokenizer_${item.vocab_size}
      outs:
        - ${root_dir}/data/models/zabantu-${lang}-${item.vocab_size}-${item.model_type}-${item.epochs}epochs

  train_nso_ven_language_model_base:
    foreach:
      - model_type: unigram
        vocab_size: 30k
      - model_type: unigram
        vocab_size: 50k
      - model_type: unigram
        vocab_size: 70k

      - model_type: bpe
        vocab_size: 30k
      - model_type: bpe
        vocab_size: 50k
      - model_type: bpe
        vocab_size: 70k
    do:
      desc: Train a languga model using the XLM-R architecture on the Tshivenda and Sepedi corpora
      vars:
        - lang: nso_ven
      cmd: >-
        cd ${root_dir} &&
        bash ${root_dir}/${mlm_scripts_dir}/run_xlm_train_pipeline.sh 
        ${root_dir}/${mlm_scripts_dir}/zabantu_blm_base.yml                                            
        zabantu-${lang}-${item.vocab_size}-${item.model_type}-${base_model_epochs}epochs           
        ${mlm_train_in_dir}                                                               
        zabantu-lm-train-${lang}-$RANDOM                                                    
        ${root_dir}/${zabantu_blmr_nso_ven_tokenizer}/zabantu_nso_ven_sent_${item.model_type}_tokenizer_${item.vocab_size}
        ${base_model_epochs}
      deps:
        - ${root_dir}/${zabantu_blmr_nso_ven_tokenizer}/zabantu_nso_ven_sent_${item.model_type}_tokenizer_${item.vocab_size}
      outs:
        - ${root_dir}/data/models/zabantu-${lang}-${item.vocab_size}-${item.model_type}-${base_model_epochs}epochs

  train_nso_ven_language_models:
    foreach:
      # - model_type: unigram
      #   vocab_size: 30k
      #   epochs: 20
      # - model_type: unigram
      #   vocab_size: 30k
      #   epochs: 50
      # - model_type: unigram
      #   vocab_size: 30k
      #   epochs: 100

      # - model_type: unigram
      #   vocab_size: 50k
      #   epochs: 20
      # - model_type: unigram
      #   vocab_size: 50k
      #   epochs: 50
      # - model_type: unigram
      #   vocab_size: 50k
      #   epochs: 100

      # - model_type: unigram
      #   vocab_size: 70k
      #   epochs: 20
      # - model_type: unigram
      #   vocab_size: 70k
      #   epochs: 50
      # - model_type: unigram
      #   vocab_size: 70k
      #   epochs: 100

      - model_type: unigram
        vocab_size: 85k
        epochs: 20

      # - model_type: bpe
      #   vocab_size: 30k
      #   epochs: 20
      # - model_type: bpe
      #   vocab_size: 30k
      #   epochs: 50
      # - model_type: bpe
      #   vocab_size: 30k
      #   epochs: 100

      # - model_type: bpe
      #   vocab_size: 50k
      #   epochs: 20
      # - model_type: bpe
      #   vocab_size: 50k
      #   epochs: 50
      # - model_type: bpe
      #   vocab_size: 50k
      #   epochs: 100

      # - model_type: bpe
      #   vocab_size: 70k
      #   epochs: 20
      # - model_type: bpe
      #   vocab_size: 70k
      #   epochs: 50
      # - model_type: bpe
      #   vocab_size: 70k
      #   epochs: 100

      - model_type: bpe
        vocab_size: 150k
        epochs: 20

      - model_type: bpe
        vocab_size: 250k
        epochs: 20

    do:
      desc: Train misc language models on the Tshivenda and Sepedi corpora using the XLM-R architecture
      vars:
        - lang: nso_ven
      cmd: >-
        cd ${root_dir} &&
        bash ${mlm_scripts_dir}/run_xlm_train_pipeline.sh 
        ${root_dir}/${mlm_scripts_dir}/zabantu_blm_base.yml                                              
        zabantu-${lang}-${item.vocab_size}-${item.model_type}-${item.epochs}epochs           
        ${mlm_train_in_dir}                                                               
        zabantu-lm-train-${lang}-$RANDOM                                                    
        ${root_dir}/${zabantu_blmr_nso_ven_tokenizer}/zabantu_nso_ven_sent_${item.model_type}_tokenizer_${item.vocab_size}
        ${item.epochs}
      deps:
        - ${root_dir}/${zabantu_blmr_nso_ven_tokenizer}/zabantu_nso_ven_sent_${item.model_type}_tokenizer_${item.vocab_size}
      outs:
        - ${root_dir}/data/models/zabantu-${lang}-${item.vocab_size}-${item.model_type}-${item.epochs}epochs

  train_sot_ven_language_model_base:
    foreach:
      - model_type: unigram
        vocab_size: 30k
      - model_type: unigram
        vocab_size: 50k
      - model_type: unigram
        vocab_size: 70k

      - model_type: bpe
        vocab_size: 30k
      - model_type: bpe
        vocab_size: 50k
      - model_type: bpe
        vocab_size: 70k
    do:
      desc: Train a language model using the XLM-R architecture on Tshivenda and Sesotho family languages
      vars:
        - lang: sot_ven
      cmd: >-
        cd ${root_dir} &&
        bash ${root_dir}/${mlm_scripts_dir}/run_xlm_train_pipeline.sh 
        ${root_dir}/${mlm_scripts_dir}/zabantu_blm_sot_base.yml                                         
        zabantu-${lang}-${item.vocab_size}-${item.model_type}-${base_model_epochs}epochs           
        ${mlm_train_in_dir}                                                               
        zabantu-lm-train-${lang}-$RANDOM                                                    
        ${root_dir}/${zabantu_xlmr_sot_ven_tokenizer}/zabantu_sot_ven_sent_${item.model_type}_tokenizer_${item.vocab_size}
        ${base_model_epochs}
      deps:
        - ${root_dir}/${zabantu_xlmr_sot_ven_tokenizer}/zabantu_sot_ven_sent_${item.model_type}_tokenizer_${item.vocab_size}
      outs:
        - ${root_dir}/data/models/zabantu-${lang}-${item.vocab_size}-${item.model_type}-${base_model_epochs}epochs

  train_sot_ven_language_models:
    foreach:
      # - model_type: unigram
      #   vocab_size: 30k
      #   epochs: 20
      # - model_type: unigram
      #   vocab_size: 30k
      #   epochs: 50
      # - model_type: unigram
      #   vocab_size: 30k
      #   epochs: 100

      # - model_type: unigram
      #   vocab_size: 50k
      #   epochs: 20
      # - model_type: unigram
      #   vocab_size: 50k
      #   epochs: 50
      # - model_type: unigram
      #   vocab_size: 50k
      #   epochs: 100

      # - model_type: unigram
      #   vocab_size: 70k
      #   epochs: 20
      # - model_type: unigram
      #   vocab_size: 70k
      #   epochs: 50
      # - model_type: unigram
      #   vocab_size: 70k
      #   epochs: 100

      # - model_type: unigram
      #   vocab_size: 85k
      #   epochs: 20
      # - model_type: unigram
      #   vocab_size: 150
      #   epochs: 20

      # - model_type: bpe
      #   vocab_size: 30k
      #   epochs: 20
      # - model_type: bpe
      #   vocab_size: 30k
      #   epochs: 50
      # - model_type: bpe
      #   vocab_size: 30k
      #   epochs: 100

      # - model_type: bpe
      #   vocab_size: 50k
      #   epochs: 20
      # - model_type: bpe
      #   vocab_size: 50k
      #   epochs: 50
      # - model_type: bpe
      #   vocab_size: 50k
      #   epochs: 100

      # - model_type: bpe
      #   vocab_size: 70k
      #   epochs: 20
      # - model_type: bpe
      #   vocab_size: 70k
      #   epochs: 50
      # - model_type: bpe
      #   vocab_size: 70k
      #   epochs: 100

      # - model_type: bpe
      #   vocab_size: 85k
      #   epochs: 20
      - model_type: bpe
        vocab_size: 150k
        epochs: 20

    do:
      desc: Train misc language models on the Tshivenda and Sesotho family corpora using the XLM-R architecture
      vars:
        - lang: sot_ven
      cmd: >-
        cd ${root_dir} &&
        bash ${mlm_scripts_dir}/run_xlm_train_pipeline.sh 
        ${root_dir}/${mlm_scripts_dir}/zabantu_blm_sot_base.yml                                             
        zabantu-${lang}-${item.vocab_size}-${item.model_type}-${item.epochs}epochs           
        ${mlm_train_in_dir}                                                               
        zabantu-lm-train-${lang}-$RANDOM                                                    
        ${root_dir}/${zabantu_xlmr_sot_ven_tokenizer}/zabantu_sot_ven_sent_${item.model_type}_tokenizer_${item.vocab_size}
        ${item.epochs}
      deps:
        - ${root_dir}/${zabantu_xlmr_sot_ven_tokenizer}/zabantu_sot_ven_sent_${item.model_type}_tokenizer_${item.vocab_size}
      outs:
        - ${root_dir}/data/models/zabantu-${lang}-${item.vocab_size}-${item.model_type}-${item.epochs}epochs

  train_sot_ven_language_model_large:
    foreach:
      - model_type: unigram
        vocab_size: 70k

    do:
      desc: Train a language model using the XLM-R architecture on Tshivenda and Sesotho family languages
      vars:
        - lang: sot_ven
      cmd: >-
        cd ${root_dir} &&
        bash ${root_dir}/${mlm_scripts_dir}/run_xlm_train_pipeline.sh 
        ${root_dir}/${mlm_scripts_dir}/zabantu_blm_sot_base.yml                                         
        zabantu-${lang}-${item.vocab_size}-${item.model_type}-100epochs-lg           
        ${mlm_train_in_dir}                                                               
        zabantu-lm-train-${lang}-$RANDOM                                                    
        ${root_dir}/${zabantu_xlmr_sot_ven_tokenizer}/zabantu_sot_ven_sent_${item.model_type}_tokenizer_${item.vocab_size}
        100
      deps:
        - ${root_dir}/${zabantu_xlmr_sot_ven_tokenizer}/zabantu_sot_ven_sent_${item.model_type}_tokenizer_${item.vocab_size}
      outs:
        - ${root_dir}/data/models/zabantu-${lang}-${item.vocab_size}-${item.model_type}-100epochs-lg

  train_zabantu_language_model_base:
    foreach:
      - model_type: unigram
        vocab_size: 30k
      - model_type: unigram
        vocab_size: 50k
      - model_type: unigram
        vocab_size: 70k

      - model_type: bpe
        vocab_size: 30k
      - model_type: bpe
        vocab_size: 50k
      - model_type: bpe
        vocab_size: 70k
    do:
      desc: Train a language model using the XLM-R architecture on all South African bantu languages
      vars:
        - lang: bantu
      cmd: >-
        cd ${root_dir} &&
        bash ${root_dir}/${mlm_scripts_dir}/run_xlm_train_pipeline.sh 
        ${root_dir}/${mlm_scripts_dir}/zabantu_xlm_base.yml                                               
        zabantu-${lang}-${item.vocab_size}-${item.model_type}-${base_model_epochs}epochs           
        ${mlm_train_in_dir}                                                               
        zabantu-lm-train-${lang}-$RANDOM                                                    
        ${root_dir}/${zabantu_xlmr_tokenizer}/zabantu_xlmr_sent_${item.model_type}_tokenizer_${item.vocab_size}
        ${base_model_epochs}
      deps:
        - ${root_dir}/${zabantu_xlmr_tokenizer}/zabantu_xlmr_sent_${item.model_type}_tokenizer_${item.vocab_size}
      outs:
        - ${root_dir}/data/models/zabantu-${lang}-${item.vocab_size}-${item.model_type}-${base_model_epochs}epochs

  train_zabantu_language_models:
    foreach:
      # - model_type: unigram
      #   vocab_size: 30k
      #   epochs: 20
      # - model_type: unigram
      #   vocab_size: 30k
      #   epochs: 50
      # - model_type: unigram
      #   vocab_size: 30k
      #   epochs: 100

      # - model_type: unigram
      #   vocab_size: 50k
      #   epochs: 20
      # - model_type: unigram
      #   vocab_size: 50k
      #   epochs: 50
      # - model_type: unigram
      #   vocab_size: 50k
      #   epochs: 100

      # - model_type: unigram
      #   vocab_size: 70k
      #   epochs: 20
      # - model_type: unigram
      #   vocab_size: 70k
      #   epochs: 50
      # - model_type: unigram
      #   vocab_size: 70k
      #   epochs: 100

      - model_type: unigram
        vocab_size: 85k
        epochs: 20
      - model_type: unigram
        vocab_size: 150k
        epochs: 20

      # - model_type: bpe
      #   vocab_size: 30k
      #   epochs: 20
      # - model_type: bpe
      #   vocab_size: 30k
      #   epochs: 50
      # - model_type: bpe
      #   vocab_size: 30k
      #   epochs: 100

      # - model_type: bpe
      #   vocab_size: 50k
      #   epochs: 20
      # - model_type: bpe
      #   vocab_size: 50k
      #   epochs: 50
      # - model_type: bpe
      #   vocab_size: 50k
      #   epochs: 100

      # - model_type: bpe
      #   vocab_size: 70k
      #   epochs: 20
      # - model_type: bpe
      #   vocab_size: 70k
      #   epochs: 50
      # - model_type: bpe
      #   vocab_size: 70k
      #   epochs: 100

      - model_type: bpe
        vocab_size: 85k
        epochs: 20
      - model_type: bpe
        vocab_size: 150
        epochs: 20

    do:
      desc: Train misc language models on all South African bantu languages using the XLM-R architecture
      vars:
        - lang: bantu
      cmd: >-
        cd ${root_dir} &&
        bash ${mlm_scripts_dir}/run_xlm_train_pipeline.sh 
        ${root_dir}/${mlm_scripts_dir}/zabantu_xlm_base.yml                                              
        zabantu-${lang}-${item.vocab_size}-${item.model_type}-${item.epochs}epochs
        ${mlm_train_in_dir}                                                               
        zabantu-lm-train-${lang}-$RANDOM                                                    
        ${root_dir}/${zabantu_xlmr_tokenizer}/zabantu_xlmr_sent_${item.model_type}_tokenizer_${item.vocab_size}
        ${item.epochs}
      deps:
        - ${root_dir}/${zabantu_xlmr_tokenizer}/zabantu_xlmr_sent_${item.model_type}_tokenizer_${item.vocab_size}
      outs:
        - ${root_dir}/data/models/zabantu-${lang}-${item.vocab_size}-${item.model_type}-${item.epochs}epochs

  train_zabantu_language_model_large:
    foreach:
      - model_type: bpe
        vocab_size: 250k

    do:
      desc: Train a language model using the XLM-R architecture on all South African bantu languages
      vars:
        - lang: bantu
      cmd: >-
        cd ${root_dir} &&
        bash ${root_dir}/${mlm_scripts_dir}/run_xlm_train_pipeline.sh 
        ${root_dir}/${mlm_scripts_dir}/zabantu_xlm_base.yml                                               
        zabantu-${lang}-${item.vocab_size}-${item.model_type}-${base_model_epochs}epochs-lg           
        ${mlm_train_in_dir}                                                               
        zabantu-lm-train-${lang}-$RANDOM                                                    
        ${root_dir}/${zabantu_xlmr_tokenizer}/zabantu_xlmr_sent_${item.model_type}_tokenizer_${item.vocab_size}
        ${base_model_epochs}
      deps:
        - ${root_dir}/${zabantu_xlmr_tokenizer}/zabantu_xlmr_sent_${item.model_type}_tokenizer_${item.vocab_size}
      outs:
        - ${root_dir}/data/models/zabantu-${lang}-${item.vocab_size}-${item.model_type}-100epochs-lg
# TODO: Post train afriberta, xlmr and afro-xlmr models